{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "#### Single layer perceptron - using sklearn\n",
    "-----------------------\n",
    "\n",
    "- `sklearn.linear_model.Perceptron`\n",
    "\n",
    "The Perceptron is another simple classification algorithm suitable for large scale learning. By default:\n",
    "\n",
    "- It does not require a learning rate.\n",
    "- It is not regularized (penalized).\n",
    "- It updates its model only on mistakes.\n",
    "\n",
    "The last characteristic implies that the Perceptron is slightly `faster `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- `penalty` {`l2`, `l1`, `elasticnet`}, `default=None` - The penalty (aka `regularization` term) to be used.\n",
    "\n",
    "- `alpha` float, default=0.0001 - Constant that multiplies the regularization term `if regularization` is used.\n",
    "\n",
    "- `l1_ratio` float, default=0.15 - The `Elastic Net` mixing parameter, with 0 <= l1_ratio <= 1. \n",
    "        - l1_ratio=0 corresponds to L2 penalty, \n",
    "        - l1_ratio=1 to L1. \n",
    "    - Only used if penalty='elasticnet'.\n",
    "\n",
    "- `fit_intercept` bool, `default=True` - Whether the intercept should be estimated or not. `If False`, the data is assumed to be already centered.\n",
    "\n",
    "- `max_iter` int, default=1000 - The maximum number of passes over the training data (aka epochs). \n",
    "\n",
    "- `tol` float, default=1e-3 - The `stopping criterion`. \n",
    "    - If it is not `None`, the iterations will stop when (`loss > previous_loss - tol`).\n",
    "\n",
    "- `shuffle` bool, default=True - Whether or not the training data should be shuffled after each epoch.\n",
    "\n",
    "- `verbose` int, default=0 - The verbosity level\n",
    "\n",
    "- `eta` - double, `default=1` - Constant by which the updates are multiplied.\n",
    "\n",
    "- `early_stopping` bool, `default=False` - Whether to use early stopping to terminate training when validation. score is not improving. \n",
    "    - If set to True, it will automatically set aside a `stratified fraction` of training data as validation and terminate training when validation score is not improving by at least tol for `n_iter_no_change` consecutive epochs.\n",
    "\n",
    "- `validation_fraction` float, default=0.1 - The proportion of training data to set aside as validation set for `early stopping`. Must be between 0 and 1. \n",
    "    - `Only used if early_stopping is True`.\n",
    "\n",
    "- `n_iter_no_change` int, default=5 - Number of iterations with no improvement to wait before early stopping.\n",
    "\n",
    "- `class_weight` dict, {class_label: weight} or “balanced”, `default=None` - Preset for the class_weight fit parameter. Weights associated with classes. \n",
    "    - If not given, all classes are supposed to have weight one.\n",
    "    - The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "    - `warm_start` - bool, `default=False` - When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load The Iris Data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create our X and y data\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (45, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the scaler, which standarizes all the features to have mean=0 and unit variance\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.78666667, 3.04761905, 3.72380952, 1.2       ]),\n",
       " array([0.62839365, 0.19239909, 2.97000454, 0.56285714]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.mean_,sc.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the scaler to the X training data\n",
    "X_train_std = sc.transform(X_train)\n",
    "\n",
    "# Apply the SAME scaler to the X test data\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a perceptron object with the parameters: \n",
    "- 40 iterations (epochs) over the data, \n",
    "- and a learning rate of 0.1\n",
    "\n",
    "-`Fit` linear model with `Stochastic Gradient Descent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.001, max_iter=90, tol=0.1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn = Perceptron(max_iter = 90, \n",
    "                 eta0     = .001,  # learning rate\n",
    "                 tol      = 0.1,   # default, 0.001\n",
    "                 random_state = 0)\n",
    "\n",
    "# Train the perceptron\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes   : \n",
      " [0 1 2]\n",
      "Weights   : \n",
      " [[-1.68198780e-05  2.38837148e-03 -1.43683035e-03 -1.59949231e-03]\n",
      " [-1.65675799e-03 -2.85518954e-03  3.20523693e-03 -1.86607436e-03]\n",
      " [-4.79366524e-04 -1.40045418e-03  3.50641868e-03  3.33227564e-03]]\n",
      "Intercept : \n",
      " [-0.001 -0.001 -0.006]\n",
      "nbr Iter  : \n",
      " 6\n"
     ]
    }
   ],
   "source": [
    "print('Classes   : \\n', ppn.classes_)\n",
    "print('Weights   : \\n', ppn.coef_)\n",
    "print('Intercept : \\n', ppn.intercept_)\n",
    "print('nbr Iter  : \\n', ppn.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Apply the trained perceptron on the X data to make predicts for the y test data\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "\n",
    "# View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### another configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "ppn = Perceptron(max_iter = 2500, \n",
    "                 tol      = 0.001, \n",
    "                 eta0     = .01, \n",
    "                 random_state=0)\n",
    "\n",
    "# Train the perceptron\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "# Apply the trained perceptron on the X data to make predicts for the y test data\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "\n",
    "# View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes   : \n",
      " [0 1 2]\n",
      "Weights   : \n",
      " [[-0.0001682   0.02388371 -0.0143683  -0.01599492]\n",
      " [-0.01656758 -0.0285519   0.03205237 -0.01866074]\n",
      " [-0.00479367 -0.01400454  0.03506419  0.03332276]]\n",
      "Intercept : \n",
      " [-0.01 -0.01 -0.06]\n",
      "Iterations: \n",
      " 6\n"
     ]
    }
   ],
   "source": [
    "print('Classes   : \\n', ppn.classes_)\n",
    "print('Weights   : \\n', ppn.coef_)\n",
    "print('Intercept : \\n', ppn.intercept_)\n",
    "print('Iterations: \\n', ppn.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "ppn = Perceptron(max_iter    = 2500, \n",
    "                 tol         = 0.001, \n",
    "                 eta0        = .001, \n",
    "                 random_state= 0,\n",
    "                 early_stopping     = True,\n",
    "                 validation_fraction= 0.1,\n",
    "                 n_iter_no_change   = 5,\n",
    "                )\n",
    "\n",
    "# Train the perceptron\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "# Apply the trained perceptron on the X data to make predicts for the y test data\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "\n",
    "# View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn.max_iter, ppn.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00181655,  0.0026055 , -0.00274379, -0.00226595],\n",
       "       [-0.00115216, -0.00217125,  0.00343734, -0.0014662 ],\n",
       "       [-0.00123626, -0.00094449,  0.00373852,  0.00319898]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
